{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection concept with water level timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walk through some of the principles used on the Anomaly Detection approach to quality control oceanographic data.\n",
    "\n",
    "## Objective:\n",
    "The Anomaly Detection technique to quality control oceanographic data (Castelao, 2020, submitted) is based on the principle of understanding the typicall behavior of the good data by considering not only the raw measurement and its characteristics (also called features).\n",
    "Here we will use a water level timeseries long enough so that we can have a fair estimate of the behavior of the water level in this station from the dataset itself.\n",
    "Also, we can show that the same Anomaly Detection principle is also valid for water level, since the original paper illustrated only for temperature profiles.\n",
    "\n",
    "Note that although the code is explicitly included in this notebook so anyone can follow step by step the procedure, it is not required to fully understand the code. The text together with the figures should make sense by themselves.\n",
    "\n",
    "This and other notebooks on quality control are available at https://cotede.castelao.net in /docs/notebooks/.\n",
    "There you can run the notebooks without installing anything in your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.models import ColumnDataSource, CustomJS, Slider\n",
    "from bokeh.plotting import figure\n",
    "from scipy import stats\n",
    "\n",
    "import cotede\n",
    "from cotede import qctests, datasets\n",
    "import pandas as pd\n",
    "from supportdata import download_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We'll use a water level sample dataset from CoTeDe for this tutorial. This is the water level station: 8764227 LAWMA, Amerada Pass, LA, operated by NOAA / NOS / CO-OPS, and kindly provided by Lindsay Abrams. If curious about it, check the documentation for more details on this data.\n",
    "\n",
    "Fortunatelly, this data was already flagged by NOAA personel, so let's take advantage of that and as use it as a reference to verify if we are doing a good job, but keep in mind that the idea is that when applying Anomaly Detection we would not expect to have the labels, i.e. we wouldn't know the answer a priori.\n",
    "\n",
    "Let's load the data and check which variables are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cotede.datasets.load_water_level()\n",
    "\n",
    "print(\"The variables are: \", data.keys())\n",
    "print(\"There is a total of {} observations.\".format(len(data[\"epoch\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data was previously quality controlled. Let's use that as our indexes of good and bad data to verify what we should be identifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_good = ~data[\"flagged\"]\n",
    "idx_bad = data[\"flagged\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A time series with the data\n",
    "# x_axis_type='datetime'\n",
    "p = figure(plot_width=750, plot_height=300, title=\"Water Level\")\n",
    "p.circle(data['epoch'][idx_good], data[\"water_level\"][idx_good], size=8, line_color=\"orange\", fill_color=\"orange\", fill_alpha=0.5, legend_label=\"Good values\")\n",
    "p.triangle(data[\"epoch\"][idx_bad], data[\"water_level\"][idx_bad], size=12, line_color=\"red\", fill_color=\"red\", fill_alpha=0.8, legend_label=\"Bad values\")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data distribution\n",
    "Based on the manual flagging, let's check the distribution of the good and bad data.\n",
    "\n",
    "The good data seems to be normally distributed. The bad values can be quite distinct to the typicall good data, showing a cluster of clear outliers above 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_good, edges_good = np.histogram(data[\"water_level\"][idx_good], density=False, bins=50)\n",
    "hist_bad, edges_bad = np.histogram(data[\"water_level\"][idx_bad], density=False, bins=50)\n",
    "\n",
    "p = figure(plot_width=750, plot_height=300, background_fill_color=\"#fafafa\", title=\"Data distribution\")\n",
    "p.quad(top=hist_good, bottom=0, left=edges_good[:-1], right=edges_good[1:],\n",
    "           fill_color=\"green\", line_color=\"white\", alpha=0.5, legend_label=\"Good data\")\n",
    "p.quad(top=hist_bad, bottom=0, left=edges_bad[:-1], right=edges_bad[1:],\n",
    "           fill_color=\"red\", line_color=\"white\", alpha=0.5, legend_label=\"Bad data\")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's estimate the mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_estimated, sigma_estimated = stats.norm.fit(data[\"water_level\"])\n",
    "\n",
    "print(\"Estimated mean: {:.3f}, and standard deviation: {:.3f}\".format(mu_estimated, sigma_estimated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ref = np.linspace(data[\"water_level\"].min(), data[\"water_level\"].max(), 1000)\n",
    "pdf = stats.norm.pdf(x_ref, loc=mu_estimated, scale=sigma_estimated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_good, edges_good = np.histogram(data[\"water_level\"][idx_good], density=True, bins=50)\n",
    "hist_bad, edges_bad = np.histogram(data[\"water_level\"][idx_bad], density=True, bins=50)\n",
    "\n",
    "p = figure(plot_width=750, plot_height=300, background_fill_color=\"#fafafa\")\n",
    "p.quad(top=hist_good, bottom=0, left=edges_good[:-1], right=edges_good[1:],\n",
    "           fill_color=\"green\", line_color=\"white\", alpha=0.5, legend_label=\"Good data\")\n",
    "p.quad(top=hist_bad, bottom=0, left=edges_bad[:-1], right=edges_bad[1:],\n",
    "           fill_color=\"red\", line_color=\"white\", alpha=0.5, legend_label=\"Bad data\")\n",
    "p.line(x_ref, pdf, line_color=\"orange\", line_width=6, alpha=0.7, legend_label=\"PDF fit\")\n",
    "\n",
    "show(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our estimated PDF doesn't look great, and that is due to the outliers. We better use a robust estimator.\n",
    "\n",
    "### Robust estimate of the mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_robust = np.percentile(data[\"water_level\"], 50)\n",
    "sigma_robust = (np.percentile(data[\"water_level\"], 75) - np.percentile(data[\"water_level\"], 25)) / 1.349\n",
    "\n",
    "print(\"Estimated robust mean: {:.3f}, and robust standard deviation: {:.3f}\".format(mu_robust, sigma_robust))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ref = np.linspace(data[\"water_level\"].min(), data[\"water_level\"].max(), 1000)\n",
    "pdf = stats.norm.pdf(x_ref, loc=mu_robust, scale=sigma_robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_good, edges_good = np.histogram(data[\"water_level\"][idx_good], density=True, bins=50)\n",
    "hist_bad, edges_bad = np.histogram(data[\"water_level\"][idx_bad], density=True, bins=50)\n",
    "\n",
    "p = figure(plot_width=750, plot_height=300, background_fill_color=\"#fafafa\", title=\"Probability Density Function\")\n",
    "p.quad(top=hist_good, bottom=0, left=edges_good[:-1], right=edges_good[1:],\n",
    "           fill_color=\"green\", line_color=\"white\", alpha=0.5, legend_label=\"Good data\")\n",
    "p.quad(top=hist_bad, bottom=0, left=edges_bad[:-1], right=edges_bad[1:],\n",
    "           fill_color=\"red\", line_color=\"white\", alpha=0.5, legend_label=\"Bad data\")\n",
    "p.line(x_ref, pdf, line_color=\"orange\", line_width=6, alpha=0.7, legend_label=\"PDF fit\")\n",
    "# p.line(x_ref, sf, line_color=\"blue\", line_width=4, alpha=0.7, legend_label=\"SF\")\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survival Function\n",
    "Once we can estimate the PDF we can also obtain the Survival Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = stats.norm.cdf(x_ref, loc=mu_robust, scale=sigma_robust)\n",
    "sf = stats.norm.sf(x_ref, loc=mu_robust, scale=sigma_robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist, edges = np.histogram(data[\"water_level\"], density=True, bins=50)\n",
    "\n",
    "p = figure(plot_width=750, plot_height=300, background_fill_color=\"#fafafa\", title=\"Survival Function\")\n",
    "# p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:],\n",
    "#            fill_color=\"dodgerblue\", line_color=\"white\", alpha=0.5)\n",
    "p.quad(top=hist_good, bottom=0, left=edges_good[:-1], right=edges_good[1:],\n",
    "           fill_color=\"green\", line_color=\"white\", alpha=0.5, legend_label=\"Good data\")\n",
    "p.quad(top=hist_bad, bottom=0, left=edges_bad[:-1], right=edges_bad[1:],\n",
    "           fill_color=\"red\", line_color=\"white\", alpha=0.5, legend_label=\"Bad data\")\n",
    "p.line(x_ref, cdf, line_color=\"lightseagreen\", line_width=4, alpha=0.7, legend_label=\"CDF\")\n",
    "p.line(x_ref, sf, line_color=\"orange\", line_width=4, alpha=0.7, legend_label=\"SF\")\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given value $x_i$, the CDF gives the probability of sampling from this dataset a value equal or smaller than $x_i$, while the SF gives the probability of sampling a value equal or greater than $x_i$.\n",
    "For instance, values higer than 8 are only 5% of the dataset (you can check it by zoom in the orange line).\n",
    "Therefore, the SF can be used as a guide on how rare is a measurement in the upper bound perspective.\n",
    "\n",
    "### Cumulative Density Function versus Survival Function\n",
    "Let's play with the good data to better understand the CDF and the SF. You don't need to fully understand the code in the next box, but you can jump straight to plot below and play with the slider for different water levels.\n",
    "\n",
    "The top plot is a normalized histogram of the observed water level flagged as good, and the line is our estimated PDF. The second plot shows the area on the histogram that is greater or smaller than the water level selected with the slider. The bottom plot shows the CDF and SF, which are equivalent to the areas in the middle plot, therefore, higer the water level choice, less orange in the histogram (middle), and smaller is the value of the SF (bottom).\n",
    "\n",
    "For the Anomaly Detection implemented to QC data, we use the SF as an index to quantify how rare is an observation in respect to the upper bound. If we were insterested in how rare is a small value (i.e. the lower bound) we should be using the CDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"water_level\"][idx_good]\n",
    "hist, edges = np.histogram(x, density=True, bins=50)\n",
    "\n",
    "\n",
    "x_ref = np.linspace(data[\"water_level\"].min(), 8.3, 250)\n",
    "\n",
    "pdf = stats.norm.pdf(x_ref, loc=mu_robust, scale=sigma_robust)\n",
    "cdf = stats.norm.cdf(x_ref, loc=mu_robust, scale=sigma_robust)\n",
    "sf = stats.norm.sf(x_ref, loc=mu_robust, scale=sigma_robust)\n",
    "\n",
    "slider = Slider(title=\"water level\", value=np.median(x), start=x.min(), end=x.max(), step=0.02, orientation=\"horizontal\")\n",
    "\n",
    "\n",
    "tmp = dict(\n",
    "    x_ref=x_ref.copy(),\n",
    "    pdf_ref=pdf.copy(),\n",
    "    cdf_ref=cdf.copy(),\n",
    "    sf_ref=sf.copy(),\n",
    "    cdf=cdf.copy(),\n",
    "    sf=sf.copy()\n",
    ")\n",
    "tmp[\"cdf\"][x_ref > slider.value] = np.nan\n",
    "tmp[\"sf\"][x_ref < slider.value] = np.nan\n",
    "\n",
    "dist_source = ColumnDataSource(data=tmp)\n",
    "\n",
    "\n",
    "tmp = dict(\n",
    "    hist=hist,\n",
    "    left=edges[:-1],\n",
    "    right=edges[1:],\n",
    "    ch=hist.copy(),\n",
    "    cl=edges[:-1].copy(),\n",
    "    cr=edges[1:].copy(),\n",
    "    sh=hist.copy(),\n",
    "    sl=edges[:-1].copy(),\n",
    "    sr=edges[1:].copy()\n",
    ")\n",
    "\n",
    "idx = edges[1:] < slider.value\n",
    "tmp[\"sh\"][idx] = np.nan\n",
    "tmp[\"sl\"][idx] = np.nan\n",
    "tmp[\"sr\"][idx] = np.nan\n",
    "idx = edges[:-1] > slider.value\n",
    "tmp[\"ch\"][idx] = np.nan\n",
    "tmp[\"cl\"][idx] = np.nan\n",
    "tmp[\"cr\"][idx] = np.nan\n",
    "\n",
    "\n",
    "source = ColumnDataSource(data=tmp)\n",
    "# source = ColumnDataSource(data=tmp)\n",
    "callback = CustomJS(args=dict(source=source, dist_source=dist_source), code=\"\"\"\n",
    "    var data = source.data;\n",
    "    var f = cb_obj.value;\n",
    "    var hist = data['hist'];\n",
    "    var left = data['left'];\n",
    "    var right = data['right'];\n",
    "    var ch = data['ch'];\n",
    "    var cl = data['cl'];\n",
    "    var cr = data['cr'];\n",
    "    var sh = data['sh'];\n",
    "    var sl = data['sl'];\n",
    "    var sr = data['sr'];\n",
    "    for (var i = 0; i < hist.length; i++) {\n",
    "        if (left[i] > f) {\n",
    "            ch[i] = \"NaN\";\n",
    "            cl[i] = \"NaN\";\n",
    "            cr[i] = \"NaN\";\n",
    "        } else {\n",
    "            ch[i] = hist[i];\n",
    "            cl[i] = left[i];\n",
    "            cr[i] = right[i];\n",
    "        }\n",
    "        if (right[i] < f) {\n",
    "            sh[i] = \"NaN\";\n",
    "            sl[i] = \"NaN\";\n",
    "            sr[i] = \"NaN\";\n",
    "        } else {\n",
    "            sh[i] = hist[i];\n",
    "            sl[i] = left[i];\n",
    "            sr[i] = right[i];\n",
    "        }\n",
    "    }\n",
    "    var ddata = dist_source.data;\n",
    "    var x_ref = ddata['x_ref'];\n",
    "    var cdf_ref = ddata['cdf_ref'];\n",
    "    var cdf = ddata['cdf'];\n",
    "    var sf_ref = ddata['sf_ref'];\n",
    "    var sf = ddata['sf'];\n",
    "    for (var i = 0; i < x_ref.length; i++) {\n",
    "        if (x_ref[i] > f) {\n",
    "            cdf[i] = \"NaN\"\n",
    "            sf[i] = sf_ref[i]\n",
    "        } else {\n",
    "            cdf[i] = cdf_ref[i]\n",
    "            sf[i] = \"NaN\"\n",
    "        }\n",
    "    }\n",
    "    dist_source.change.emit();\n",
    "    source.change.emit();\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "slider.js_on_change('value', callback)\n",
    "\n",
    "p_top = figure(plot_width=750, plot_height=300, background_fill_color=\"#fafafa\", title=\"Probability Density Function\")\n",
    "p_top.quad(top=\"hist\", bottom=0, left=\"left\", right=\"right\", source=source,\n",
    "           fill_color=\"green\", line_color=\"white\", alpha=0.5)\n",
    "p_top.line(x_ref, pdf, line_color=\"crimson\", line_width=6, alpha=0.7, legend_label=\"PDF fit\")\n",
    "\n",
    "\n",
    "p1 = figure(plot_width=750, plot_height=300, background_fill_color=\"#fafafa\", title=\"Observations in respect to the threshold\")\n",
    "p1.quad(top=\"ch\", bottom=0, left=\"cl\", right=\"cr\", source=source,\n",
    "           fill_color=\"lightseagreen\", line_color=\"white\", alpha=0.5, legend_label=\"lower than\")\n",
    "p1.quad(top=\"sh\", bottom=0, left=\"sl\", right=\"sr\", source=source,\n",
    "           fill_color=\"orange\", line_color=\"white\", alpha=0.5, legend_label=\"greater than\")\n",
    "p1.line(x_ref, pdf, line_color=\"crimson\", line_width=6, alpha=0.7, legend_label=\"PDF fit\")\n",
    "\n",
    "p2 = figure(plot_width=750, plot_height=300, background_fill_color=\"#fafafa\", title=\"Cummulative Density Function & Survival Function\")\n",
    "p2.x_range = p1.x_range\n",
    "p2.line(\"x_ref\", \"cdf_ref\", source=dist_source, line_color=\"lightgray\", line_width=1, alpha=0.7, legend_label=\"CDF\")\n",
    "p2.line(\"x_ref\", \"cdf\", source=dist_source, line_color=\"lightseagreen\", line_width=4, alpha=0.7, legend_label=\"CDF\")\n",
    "p2.line(\"x_ref\", \"sf_ref\", source=dist_source, line_color=\"lightgray\", line_width=1, alpha=0.7, legend_label=\"CDF\")\n",
    "p2.line(\"x_ref\", \"sf\", source=dist_source, line_color=\"orange\", line_width=4, alpha=0.7, legend_label=\"SF\")\n",
    "\n",
    "p = column(p_top,slider, p1, p2)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How rare is to observe in this station a water level higher than 8? And higher than 10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 8.1\n",
    "x2 = 10\n",
    "print(\"While the SF({})={:.3}, the SF({})={:.3e}\".format(\n",
    "    x1,\n",
    "    stats.norm.sf(x1, loc=mu_robust, scale=sigma_robust),\n",
    "    x2,\n",
    "    stats.norm.sf(x2, loc=mu_robust, scale=sigma_robust)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that a sea level 10 is less common than 8.1, but how much less? The survival function is a way to scale that, by defining how frequently it was observed values equal or higher than the one in question. For instance, 2.5% of the observations were equal or higher than 8.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a new perspective: Spikiness using Tukey53H\n",
    "Looking back on the distribution plots it is clear there are bad measurements within the scale of valid measurements, i.e. between 7 and 8.\n",
    "Just looking at the magnitude it is not possible to identify those, so we shall add more tests.\n",
    "Let's start with Tukey53H (if curious, check CoTeDe's manual (https://cotede.castelao.net) about this procedure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tukey53H = qctests.tukey53H(data[\"water_level\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look on the same timeseries but projected as a Tukey53H.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A time series with the data\n",
    "p1 = figure(plot_width=750, plot_height=300, title=\"Water Level\")\n",
    "p1.circle(data['epoch'][idx_good], data[\"water_level\"][idx_good], size=8, line_color=\"orange\", fill_color=\"orange\", fill_alpha=0.5, legend_label=\"Good values\")\n",
    "p1.triangle(data[\"epoch\"][idx_bad], data[\"water_level\"][idx_bad], size=12, line_color=\"red\", fill_color=\"red\", fill_alpha=0.8, legend_label=\"Bad values\")\n",
    "\n",
    "p2 = figure(plot_width=750, plot_height=300, title=\"Tukey53H of the water level\")\n",
    "p2.x_range = p1.x_range\n",
    "p2.circle(data['epoch'][idx_good], y_tukey53H[idx_good], size=8, line_color=\"orange\", fill_color=\"orange\", fill_alpha=0.5, legend_label=\"Good values\")\n",
    "p2.triangle(data[\"epoch\"][idx_bad], y_tukey53H[idx_bad], size=12, line_color=\"red\", fill_color=\"red\", fill_alpha=0.8, legend_label=\"Bad values\")\n",
    "p = column(p1, p2)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the distribution of the tukey53H feature?\n",
    "Different than the raw data, the tukey53H of the good data is quite different than the bad data.\n",
    "For that reason, let's use two plots, with different scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.isfinite(y_tukey53H)\n",
    "\n",
    "hist_good, edges_good = np.histogram(y_tukey53H[idx & idx_good], density=False, bins=50)\n",
    "\n",
    "p1 = figure(plot_width=750, plot_height=300, background_fill_color=\"#fafafa\", title=\"Tukey53H of the good data\")\n",
    "p1.quad(top=hist_good, bottom=0, left=edges_good[:-1], right=edges_good[1:],\n",
    "           fill_color=\"green\", line_color=\"white\", alpha=0.5)\n",
    "\n",
    "hist_bad, edges_bad = np.histogram(y_tukey53H[idx & idx_bad], density=False, bins=50)\n",
    "\n",
    "p2 = figure(plot_width=750, plot_height=300, background_fill_color=\"#fafafa\", title=\"Tukey53H of the bad data\")\n",
    "p2.quad(top=hist_bad, bottom=0, left=edges_bad[:-1], right=edges_bad[1:],\n",
    "           fill_color=\"red\", line_color=\"white\", alpha=0.5)\n",
    "\n",
    "p = column(p1, p2)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a robust estimate of the mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_tukey53H = np.percentile(y_tukey53H[idx], 50)\n",
    "sigma_tukey53H = (np.percentile(y_tukey53H[idx], 75) - np.percentile(y_tukey53H[idx], 25)) / 1.349\n",
    "\n",
    "print(\"Estimated robust mean: {:.3e}, and robust standard deviation: {:.3e}\".format(mu_tukey53H, sigma_tukey53H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does that compares with a non robust estimate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_tukey53H, sigma_tukey53H = stats.norm.fit(y_tukey53H[idx])\n",
    "\n",
    "print(\"Estimated mean: {:.3f}, and standard deviation: {:.3f}\".format(mu_estimated, sigma_estimated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"While the SF(0.01)={:.3}, the SF(0.5)={:.3e}\".format(\n",
    "    stats.norm.sf(0.01, loc=mu_tukey53H, scale=sigma_tukey53H),\n",
    "    stats.norm.sf(0.5, loc=mu_tukey53H, scale=sigma_tukey53H)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining tests into a multidimensional criterion\n",
    "How could we aggregate information from multiple tests into a combined criterion? Extreme cases are easy to identify, such the water elevation above 14, but as we get closer to expected values it is harder to decide based in a single criteria without compromising by also flagging good data as bad.\n",
    "One alternative is to combine multiple perspectives which alone are not clear but combined could make a clear case.\n",
    "\n",
    "Here we used two features to evaluate each measurement, the water elevation itself and the Tukey53H of the water elevation.\n",
    "Those have different scales (compare the histograms shown before), so those can't be combined without some sort of scaling.\n",
    "A difference in 0.1 in the water level is not the same effect of a difference in 0.1 in Tukey53H.\n",
    "One way to normalize each feature is by using the survival function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_i = 8.1\n",
    "pxi = stats.norm.sf(x_i, loc=mu_robust, scale=sigma_robust)\n",
    "\n",
    "y_i = 0.1\n",
    "pyi = stats.norm.sf(y_i, loc=mu_tukey53H, scale=sigma_tukey53H)\n",
    "\n",
    "print(\"\"\"The probability of observing a measurement higher than {} is {:.3e}.\\n\"\"\".format(x_i, pxi))\n",
    "print(\"\"\"And the probability of observing a tukey53H value higher than {} is {:.3e}.\\n\"\"\".format(y_i, pyi))\n",
    "\n",
    "print(\"If we assume that these are independent processes, i.e. a spike is independent of the actual water level thus it can happend at any point, the probability of a tukey53H larger than {} while resulting in a value higher than {} is {:.3e}\"\"\".format(y_i, x_i, pxi * pyi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on the uncommon\n",
    "Instead of assuming that all features have a normal distribution, and trying to fit the whole dataset, let's focus on the extremme cases.\n",
    "A good observing system has typically much less than 1% of its measurements invalid, so let's fit our PDF using only the top 5% values.\n",
    "\n",
    "Let's reconsider the feature Tukey53H, and assume it is symmetric, i.e. doesn't matter if the spike is up or down, which is not always true. For instance spikes on Chlorophyll measurements are not symmetric, thus positive spikes are different then negative ones.\n",
    "Since we are looking at the tail, instead of a Gaussian, let's use an Exponential Weibull distribution which will give us more degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tukey53H = np.absolute(qctests.tukey53H(data[\"water_level\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tukey53H_top95 = np.percentile(y_tukey53H[np.isfinite(y_tukey53H)], 95)\n",
    "\n",
    "print(\"The 95 percentile of the valid absolute Tukey53H is {:.4e}\".format(tukey53H_top95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take only the top 5% of Tukey53H values and fit an exponential weibull distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_tukey53H[y_tukey53H > tukey53H_top95]\n",
    "\n",
    "from scipy.stats import exponweib\n",
    "param_tukey53H = exponweib.fit(y)\n",
    "param_tukey53H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ref = np.linspace(y.min(), y.max(), 1000)\n",
    "sf = exponweib.sf(y_ref, *param_tukey53H[:-2], loc=param_tukey53H[-2], scale=param_tukey53H[-1])\n",
    "\n",
    "# An index where Tukey53H is valid. The tails can't be calculated (check CoTeDe's manual)\n",
    "idx = np.isfinite(y_tukey53H)\n",
    "\n",
    "hist_good, edges_good = np.histogram(y_tukey53H[idx & idx_good], density=False, bins=50)\n",
    "\n",
    "p1 = figure(plot_width=750, plot_height=250, background_fill_color=\"#fafafa\")\n",
    "p1.quad(top=hist_good, bottom=0, left=edges_good[:-1], right=edges_good[1:],\n",
    "           fill_color=\"green\", line_color=\"white\", alpha=0.5)\n",
    "\n",
    "hist_bad, edges_bad = np.histogram(y_tukey53H[idx & idx_bad], density=False, bins=50)\n",
    "\n",
    "p2 = figure(plot_width=750, plot_height=250, background_fill_color=\"#fafafa\")\n",
    "p2.quad(top=hist_bad, bottom=0, left=edges_bad[:-1], right=edges_bad[1:],\n",
    "           fill_color=\"red\", line_color=\"white\", alpha=0.5)\n",
    "\n",
    "p3 = figure(plot_width=750, plot_height=250, background_fill_color=\"#fafafa\", title=\"Survival Function\")\n",
    "p3.line(y_ref, sf, line_color=\"orange\", line_width=4, alpha=0.7)\n",
    "\n",
    "p = column(p1, p2, p3)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for the water level itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top95 = np.percentile(data[\"water_level\"], 95)\n",
    "print(\"The 95 percentile of the valid water level is {:.5f}\".format(top95))\n",
    "\n",
    "x = data[\"water_level\"][data[\"water_level\"] > top95]\n",
    "\n",
    "param = exponweib.fit(x)\n",
    "\n",
    "x1 = 8.1\n",
    "x2 = 10\n",
    "print(\"While the SF({})={:.3}, the SF({})={:.3}\".format(\n",
    "    x1,\n",
    "    exponweib.sf(x1, *param[:-2], loc=param[-2], scale=param[-1]),\n",
    "    x2,\n",
    "    exponweib.sf(x2, *param[:-2], loc=param[-2], scale=param[-1])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine both probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.arange(7, 14, 0.05)\n",
    "yy = np.arange(0, 1, 0.005)\n",
    "X, Y = np.meshgrid(xx, yy)\n",
    "\n",
    "P = exponweib.sf(X, *param[:-2], loc=param[-2], scale=param[-1]) * exponweib.sf(Y, *param_tukey53H[:-2], loc=param_tukey53H[-2], scale=param_tukey53H[-1])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.contour(X, Y, np.log(P))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
